<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>MineArena Voice Casting</title>
  <style>
    body { 
      font-family: system-ui, sans-serif; 
      padding: 20px; 
      max-width: 600px; 
      margin: 0 auto;
      background: #1a1a1a;
      color: #ffffff;
      font-size: 18px;
    }
    .container {
      background: #2a2a2a;
      border-radius: 12px;
      padding: 32px;
      text-align: center;
    }
    h1 {
      margin: 0 0 24px 0;
      color: #4CAF50;
      font-size: 2.2em;
    }
    .status {
      background: #333;
      padding: 20px;
      border-radius: 8px;
      margin: 24px 0;
      border-left: 6px solid #4CAF50;
      font-size: 1.1em;
      line-height: 1.5;
    }
    .status.listening {
      border-left-color: #FF5722;
      background: #2d1b1b;
    }
    .status.error {
      border-left-color: #f44336;
      background: #2d1b1b;
    }

  </style>
</head>
<body>
  <div class="container">
    <h1>üé§ MineArena Voice Casting</h1>
    <div id="status" class="status">Connecting...</div>
  </div>

<script>
(() => {
  // ====== CONFIG ======
  let lang = "en-US";
  // Array of entries: { phrase: string, name: string }
  let spells = [];
  let isListening = false;
  
  const params = new URLSearchParams(location.search);
  const wsPort = Number(params.get("wsPort") || "8765");
  const WS_URL = `ws://127.0.0.1:${wsPort}/bridge`;

  // DOM elements
  const statusEl = document.getElementById("status");

  // ====== WS BRIDGE ======
  let ws;
  function send(msg) { 
    if (ws && ws.readyState === 1) {
      ws.send(JSON.stringify(msg)); 
    }
  }
  
  function connectWS() {
    ws = new WebSocket(WS_URL);
    ws.onopen = () => {
      setStatus("Please accept microphone permission to cast spells...");
      // Auto-start speech recognition once connected
      startRecognition();
      // Notify sidecar that the page is ready to receive config
      send({ type: "ready" });
    };
    
    ws.onmessage = (evt) => {
      const msg = JSON.parse(evt.data || "{}");
      console.log("[Voice Debug] Received message:", msg);
      
      if (msg["type"] === "config") {
        console.log("[Voice Debug] Config received - lang:", msg.lang, "spells:", msg.spells);
        if (msg.lang) lang = msg.lang;
        console.log("[Voice Debug] Lang:", lang);
        if (Array.isArray(msg.spells)) {
          // Expect array of { phrase, name }
          console.log("[Voice Debug] Updated spells array:", msg.spells);
          spells = msg.spells.filter(s => s && typeof s.phrase === 'string' && typeof s.name === 'string');
          // No need to restart - just update the spell list
        }
      } else if (msg["type"] === "end") {
        console.log("[Voice Debug] Received end message - closing tab");
        // Stop speech recognition
        isListening = false;
        if (recog) {
          try {
            recog.stop();
          } catch (e) {
            console.log("[Voice Debug] Error stopping recognition:", e);
          }
        }
        // Close the tab (may be blocked by browser if not user-initiated)
        const closed = window.close();
        setTimeout(() => {
          try { if (ws && ws.readyState === 1) ws.close(); } catch {}
          setStatus("‚úì Voice casting stopped - you can close this tab now", false);
        }, 50);
      }
    };
    
    ws.onclose = () => { 
      setStatus("Reconnecting...", false); 
      setTimeout(connectWS, 1000); 
    };
    
    ws.onerror = () => setStatus("Connection error", true);
  }
  connectWS();

  // ====== SPEECH RECOGNITION ======
  const SR = window.SpeechRecognition || window.webkitSpeechRecognition;

  let recog = null;
  
  /**
   * Deduplication strategy for interim results:
   * 
   * Problem: Interim results accumulate text as you speak:
   *   Event 1: "fire"
   *   Event 2: "fireball"           <- triggers spell
   *   Event 3: "fireball and then"  <- would trigger again! (bad)
   * 
   * Solution: Track how much of each result we've already processed.
   * For each result index, we store the character length we've processed.
   * We only check the NEW portion (substring from last processed length).
   * 
   * Example:
   *   Event 1: fullTranscript="fire", lastProcessed=0, newPortion="fire" -> no match
   *   Event 2: fullTranscript="fireball", lastProcessed=0, newPortion="fireball" -> match! Set processed=8
   *   Event 3: fullTranscript="fireball and then", lastProcessed=8, newPortion=" and then" -> no match
   * 
   * When result becomes final, we clean up tracking for that index.
   */
  // Map: resultIndex -> last character index we've processed
  let processedUpTo = new Map();

  function setStatus(text, isError = false) { 
    statusEl.textContent = text; 
    statusEl.style.color = isError ? "#ff4444" : "#ffffff";
  }

  // ===================== Normalization helpers =====================

  /**
   * normalize(s)
   *  - Lowercase
   *  - Keep letters (a‚Äìz), digits (0‚Äì9), spaces, and apostrophes.
   *    The regex /[^a-z0-9\s']/g matches any character NOT in that set and
   *    replaces it with a single space.
   *  - Collapse multiple spaces -> one space
   *  - Trim leading/trailing spaces
   */
  function normalize(s) {
    return String(s)
      .toLowerCase()
      .replace(/[^a-z0-9\s']/g, " ") // keep a-z, 0-9, spaces, apostrophes
      .replace(/\s+/g, " ")          // collapse runs of whitespace
      .trim();
  }

  /** tokenize(s): normalize + split on single spaces */
  function tokenize(s) {
    const n = normalize(s);
    return n ? n.split(" ") : [];
  }

  /** joinTokens(tokens): join tokens with single spaces to compare phrases */
  function joinTokens(toks) {
    return toks.join(" ");
  }

  // ===================== Levenshtein distance =====================
  
  function levenshtein(a, b) {
    if (a.length === 0) return b.length;
    if (b.length === 0) return a.length;
    
    const matrix = [];
    for (let i = 0; i <= b.length; i++) {
      matrix[i] = [i];
    }
    for (let j = 0; j <= a.length; j++) {
      matrix[0][j] = j;
    }
    
    for (let i = 1; i <= b.length; i++) {
      for (let j = 1; j <= a.length; j++) {
        if (b.charAt(i - 1) === a.charAt(j - 1)) {
          matrix[i][j] = matrix[i - 1][j - 1];
        } else {
          matrix[i][j] = Math.min(
            matrix[i - 1][j - 1] + 1, // substitution
            matrix[i][j - 1] + 1,     // insertion
            matrix[i - 1][j] + 1      // deletion
          );
        }
      }
    }
    
    return matrix[b.length][a.length];
  }

  // ===================== Core matching logic =====================

  const MAX_EDITS = 1; // Allow at most 1 total character edit (sub/ins/del)

  /**
   * tryMatchSpan(spanTokens, spellNorm)
   *  - spanTokens: contiguous tokens from the HEARD text (already normalized)
   *  - spellNorm:  normalized spell string (spaces preserved)
   *
   * Checks two things:
   *   1) No-merge: span tokens as-is vs spell (<= MAX_EDITS)
   *   2) Single-merge: try compacting exactly ONE adjacent pair inside the span
   *      (remove the space between tokens[j] and tokens[j+1]) and compare (<=1 edit).
   *
   * Returns true if either passes, false otherwise.
   */
  function tryMatchSpan(spanTokens, spellNorm) {
    // 1) No merge
    const phrase = joinTokens(spanTokens);
    if (levenshtein(phrase, spellNorm) <= MAX_EDITS) return true;

    // 2) Exactly one adjacent merge (remove the space once)
    if (spanTokens.length >= 2) {
      for (let j = 0; j < spanTokens.length - 1; j++) {
        const merged = [
          ...spanTokens.slice(0, j),
          spanTokens[j] + spanTokens[j + 1], // compact these two
          ...spanTokens.slice(j + 2)
        ];
        const mergedPhrase = joinTokens(merged);
        if (levenshtein(mergedPhrase, spellNorm) <= MAX_EDITS) return true;
      }
    }

    return false;
  }

  /**
   * matchSpell(hypotheses, spells)
   *  - hypotheses: array of transcript strings (best first, then alternatives)
   *  - spells: array of spell strings (may be 1+ words)
   *
   * Returns the matched spell string or null. Binary decision ‚Äî match or not.
   *
   * Matching strategy:
   *  - Checks ALL hypotheses for a match
   *  - Returns immediately on first match found (tries best hypothesis first)
   *  - Never modifies/compacts the spell text
   *  - Only compacts ONE adjacent pair in the HEARD phrase (tested exhaustively)
   *  - For a spell with m tokens, considers heard spans of length m (no-merge)
   *    and length m+1 (one merge reduces it back to m)
   */
  function matchSpell(hypotheses, spells) {
    // Pre-normalize spells once
    const spellInfos = spells.map(s => {
      const norm = normalize(s);
      return { orig: s, norm, tokens: norm ? norm.split(" ") : [] };
    });

    // Try each hypothesis in order (best first)
    for (const hypothesis of hypotheses) {
      const heardTokens = tokenize(hypothesis);
      if (!heardTokens.length) continue;

      for (const sp of spellInfos) {
        const m = sp.tokens.length;
        if (m === 0) continue;

        // A) spans of length m (no-merge path allowed inside tryMatchSpan)
        for (let i = 0; i + m <= heardTokens.length; i++) {
          const span = heardTokens.slice(i, i + m);
          if (tryMatchSpan(span, sp.norm)) return sp.orig;
        }

        // B) spans of length m+1 (exactly one merge will reduce to m)
        for (let i = 0; i + m + 1 <= heardTokens.length; i++) {
          const span = heardTokens.slice(i, i + m + 1);
          if (tryMatchSpan(span, sp.norm)) return sp.orig;
        }
      }
    }

    return null;
  }

  function startRecognition() {
    if (!SR) { 
      setStatus("‚ö†Ô∏è Speech recognition not supported in this browser - please use Chrome, Edge, or Safari", true);
      return; 
    }
    
    if (!recog) {
      recog = new SR();
      recog.continuous = true;
      recog.interimResults = true;
      recog.lang = lang;

      /**
       * SpeechRecognition result event structure:
       * 
       * event = {
       *   resultIndex: 2,                         // Index of first NEW result in this event
       *   results: [                              // SpeechRecognitionResultList - accumulates over time in continuous mode
       *     SpeechRecognitionResult {...},        // Previous utterance (already processed)
       *     SpeechRecognitionResult {...},        // Previous utterance (already processed)
       *     SpeechRecognitionResult {             // NEW utterance (what was just said) ‚Üê start here
       *       isFinal: true,
       *       length: 3,                          // Number of alternative interpretations
       *       [0]: { transcript: "fire ball", confidence: 0.95 },  // Best hypothesis
       *       [1]: { transcript: "fireball", confidence: 0.85 },   // 2nd best
       *       [2]: { transcript: "fire call", confidence: 0.60 }   // 3rd best
       *     }
       *   ]
       * }
       * 
       * We extract: allHypotheses = ["fire ball", "fireball", "fire call"]
       * 
       * Note: continuous=true means recognition keeps going after each utterance.
       * resultIndex tells us which results are new, so we only process those.
       */
      recog.onresult = (event) => {
        // Iterate through each NEW utterance result (usually just one per event)
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const utteranceResult = event.results[i];  // One SpeechRecognitionResult
          console.log("utteranceResult", utteranceResult);
          
          // Get the best hypothesis transcript
          const fullTranscript = utteranceResult[0]?.transcript || "";
          
          // Check how much of this result we've already processed
          const lastProcessedLength = processedUpTo.get(i) || 0;
          
          // Only process the NEW portion of the transcript
          const newPortion = fullTranscript.substring(lastProcessedLength);
          
          if (!newPortion.trim()) {
            // No new content to process
            continue;
          }
          
          console.log(`Result ${i}: full="${fullTranscript}", lastProcessed=${lastProcessedLength}, newPortion="${newPortion}"`);
          
          // Collect all hypotheses (n-best list) as simple strings
          // Index 0 = best (highest confidence), then 2nd best, 3rd best, etc.
          const allHypotheses = [];
          // Iterate over all hypotheses
          for (let k = 0; k < utteranceResult.length; k++) {
            const transcript = utteranceResult[k].transcript.trim();
            if (transcript) {
              // Also extract just the new portion from alternatives
              const altNewPortion = transcript.substring(lastProcessedLength);
              if (altNewPortion.trim()) {
                allHypotheses.push(altNewPortion);
              }
            }
          }
          
          if (allHypotheses.length === 0) {
            continue;
          }
          
          console.log("allHypotheses (new portions)", allHypotheses, "spells", spells);
          
          // Match against phrases, then map to the spell name to send back
          // If ANY hypothesis matches a spell (within edit distance), we cast it
          const phrases = spells.map(s => s.phrase);
          const matchedPhrase = matchSpell(allHypotheses, phrases);
          if (matchedPhrase) {
            const entry = spells.find(s => s.phrase === matchedPhrase);
            if (entry) {
              const spellName = entry.name;
              send({ type: "spellCast", spellName });
              
              // Mark that we've processed up to the end of the current full transcript
              // This prevents re-triggering if the user continues speaking
              processedUpTo.set(i, fullTranscript.length);
              console.log(`Matched "${matchedPhrase}", marked result ${i} processed up to ${fullTranscript.length}`);
            }
          }
          
          // If this is a final result, clean up the tracking for this result
          if (utteranceResult.isFinal) {
            processedUpTo.delete(i);
          }
        }
      };

      recog.onstart = () => {
        isListening = true;
        // Clear any stale processing state from previous session
        processedUpTo.clear();
        setStatus(`üéÆ Listening for spells! Return to game - this tab will close automatically when you exit the world.`);
      };

      recog.onend = () => {
        if (isListening) {
          setTimeout(() => { 
            try { 
              if (isListening) recog.start(); 
            } catch {} 
          }, 200);
        }
      };

      recog.onerror = (e) => {
        if (e.error === 'not-allowed') {
          setStatus("‚ö†Ô∏è Microphone permission denied - please allow microphone access and refresh this page to cast spells", true);
          isListening = false;
        }
      };
    }

    recog.lang = lang;
    try { 
      recog.start(); 
    } catch (e) {
      console.log("[Voice Debug] Error starting recognition:", e);
      setStatus("‚ö†Ô∏è Failed to start microphone - please check your browser permissions", true);
    }
  }



  // Auto-start when page loads
  // Speech recognition will start automatically when WebSocket connects
})();
</script>
</body>
</html> 
